{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3YoPTItNSdo",
        "outputId": "11a3d664-970a-47db-ee6c-b0351cb8dd00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting medmnist\n",
            "  Downloading medmnist-2.2.3-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.2.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from medmnist) (0.19.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from medmnist) (4.66.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from medmnist) (9.4.0)\n",
            "Collecting fire (from medmnist)\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from medmnist) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from medmnist) (0.16.0+cu118)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->medmnist) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->medmnist) (2.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2023.3.post1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (3.2.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (2023.9.26)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (1.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (23.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->medmnist) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->medmnist) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->medmnist) (2.31.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->medmnist) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->medmnist) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->medmnist) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->medmnist) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->medmnist) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->medmnist) (1.3.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116934 sha256=38ab7ed36e2712b6a7f3cbd1c66a1f8e1291e760b342a2bb46d99bd60a33e10f\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
            "Successfully built fire\n",
            "Installing collected packages: fire, medmnist\n",
            "Successfully installed fire-0.5.0 medmnist-2.2.3\n",
            "Collecting shap\n",
            "  Downloading shap-0.44.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (533 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m533.5/533.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (1.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (23.2)\n",
            "Collecting slicer==0.0.7 (from shap)\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.58.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2023.3.post1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.44.0 slicer-0.0.7\n"
          ]
        }
      ],
      "source": [
        "!pip install medmnist\n",
        "!pip install shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZA86ocnNNfKd"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "import shap\n",
        "\n",
        "import medmnist\n",
        "from medmnist import INFO, Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thQMBHW2NpOG"
      },
      "source": [
        "# Work on a 2D dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7yV_gvVNjmM"
      },
      "outputs": [],
      "source": [
        "data_flag = 'pathmnist'\n",
        "# data_flag = 'breastmnist'\n",
        "download = True\n",
        "\n",
        "NUM_EPOCHS = 3\n",
        "BATCH_SIZE = 128\n",
        "lr = 0.0015\n",
        "\n",
        "info = INFO[data_flag]\n",
        "task = info['task']\n",
        "n_channels = info['n_channels']\n",
        "n_classes = len(info['label'])\n",
        "\n",
        "DataClass = getattr(medmnist, info['python_class'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i0t1n0mNvdh"
      },
      "source": [
        "#First, we read the MedMNIST data, preprocess them and encapsulate them into dataloader form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4yVNUiQNxkI",
        "outputId": "928f589e-3d70-455f-fb05-f91932bae8c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://zenodo.org/records/6496656/files/pathmnist.npz to /root/.medmnist/pathmnist.npz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 205615438/205615438 [00:07<00:00, 26030112.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: /root/.medmnist/pathmnist.npz\n",
            "Using downloaded and verified file: /root/.medmnist/pathmnist.npz\n"
          ]
        }
      ],
      "source": [
        "# preprocessing\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[.5], std=[.5])\n",
        "])\n",
        "\n",
        "# load the data\n",
        "train_dataset = DataClass(split='train', transform=data_transform, download=download)\n",
        "test_dataset = DataClass(split='test', transform=data_transform, download=download)\n",
        "\n",
        "pil_dataset = DataClass(split='train', download=download)\n",
        "\n",
        "# encapsulate data into dataloader form\n",
        "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "train_loader_at_eval = data.DataLoader(dataset=train_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n",
        "test_loader = data.DataLoader(dataset=test_dataset, batch_size=2*BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_yMkpuSOIOL"
      },
      "source": [
        "#ResNet-18 and ResNet-50 models (for small-image datasets like CIFAR-10/100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPHSHQQPOFkQ"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Adapted from kuangliu/pytorch-cifar .\n",
        "'''\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, in_channels=1, num_classes=2):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18(in_channels, num_classes):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], in_channels=in_channels, num_classes=num_classes)\n",
        "\n",
        "\n",
        "def ResNet50(in_channels, num_classes):\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3], in_channels=in_channels, num_classes=num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E957QdSSUjSS",
        "outputId": "81a61db1-345f-4573-c020-89217f437850"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/101.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m92.2/101.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboardX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhN4hc_4OdV3"
      },
      "source": [
        "#Next, we can start to train and evaluate!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l43LT0InOeeX",
        "outputId": "99527ed4-e7b2-41df-bb11-184cb1e5c927"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Preparing data...\n",
            "Using downloaded and verified file: /root/.medmnist/pathmnist.npz\n",
            "Using downloaded and verified file: /root/.medmnist/pathmnist.npz\n",
            "Using downloaded and verified file: /root/.medmnist/pathmnist.npz\n",
            "==> Building and training model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import os\n",
        "import time\n",
        "from collections import OrderedDict\n",
        "from copy import deepcopy\n",
        "import sys\n",
        "\n",
        "import medmnist\n",
        "import numpy as np\n",
        "import PIL\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "from medmnist import INFO, Evaluator\n",
        "# from models import ResNet18, ResNet50\n",
        "from tensorboardX import SummaryWriter\n",
        "from torchvision.models import resnet18, resnet50\n",
        "from tqdm import trange\n",
        "\n",
        "\n",
        "def main(data_flag, output_root, num_epochs, gpu_ids, batch_size, download, model_flag, resize, as_rgb, model_path, run):\n",
        "\n",
        "    train_loss_list = []\n",
        "    test_loss_list = []\n",
        "    train_auc_list = []\n",
        "    train_acc_list = []\n",
        "    test_auc_list = []\n",
        "    test_acc_list = []\n",
        "    \n",
        "    lr = 0.001\n",
        "    gamma=0.1\n",
        "    milestones = [0.5 * num_epochs, 0.75 * num_epochs]\n",
        "\n",
        "    info = INFO[data_flag]\n",
        "    task = info['task']\n",
        "    n_channels = 3 if as_rgb else info['n_channels']\n",
        "    n_classes = len(info['label'])\n",
        "\n",
        "    DataClass = getattr(medmnist, info['python_class'])\n",
        "\n",
        "\n",
        "# # 直接设置设备为CPU\n",
        "#     device = torch.device('cpu')\n",
        "    str_ids = gpu_ids.split(',')\n",
        "    gpu_ids = []\n",
        "    for str_id in str_ids:\n",
        "        id = int(str_id)\n",
        "        if id >= 0:\n",
        "            gpu_ids.append(id)\n",
        "    if len(gpu_ids) > 0:\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(gpu_ids[0])\n",
        "\n",
        "    device = torch.device('cuda:{}'.format(gpu_ids[0])) if gpu_ids else torch.device('cpu') \n",
        "    \n",
        "\n",
        "    output_root = os.path.join(output_root, data_flag, time.strftime(\"%y%m%d_%H%M%S\"))\n",
        "    if not os.path.exists(output_root):\n",
        "        os.makedirs(output_root)\n",
        "\n",
        "    print('==> Preparing data...')\n",
        "\n",
        "    if resize:\n",
        "        data_transform = transforms.Compose(\n",
        "            [transforms.Resize((224, 224), interpolation=PIL.Image.NEAREST),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[.5], std=[.5])])\n",
        "    else:\n",
        "        data_transform = transforms.Compose(\n",
        "            [transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[.5], std=[.5])])\n",
        "\n",
        "    train_dataset = DataClass(split='train', transform=data_transform, download=download, as_rgb=as_rgb)\n",
        "    val_dataset = DataClass(split='val', transform=data_transform, download=download, as_rgb=as_rgb)\n",
        "    test_dataset = DataClass(split='test', transform=data_transform, download=download, as_rgb=as_rgb)\n",
        "\n",
        "\n",
        "    train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                                batch_size=batch_size,\n",
        "                                shuffle=True)\n",
        "    train_loader_at_eval = data.DataLoader(dataset=train_dataset,\n",
        "                                batch_size=batch_size,\n",
        "                                shuffle=False)\n",
        "    val_loader = data.DataLoader(dataset=val_dataset,\n",
        "                                batch_size=batch_size,\n",
        "                                shuffle=False)\n",
        "    test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                                batch_size=batch_size,\n",
        "                                shuffle=False)\n",
        "\n",
        "    print('==> Building and training model...')\n",
        "\n",
        "\n",
        "    if model_flag == 'resnet18':\n",
        "        model =  resnet18(pretrained=False, num_classes=n_classes) if resize else ResNet18(in_channels=n_channels, num_classes=n_classes)\n",
        "    elif model_flag == 'resnet50':\n",
        "        model =  resnet50(pretrained=False, num_classes=n_classes) if resize else ResNet50(in_channels=n_channels, num_classes=n_classes)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    train_evaluator = medmnist.Evaluator(data_flag, 'train')\n",
        "    val_evaluator = medmnist.Evaluator(data_flag, 'val')\n",
        "    test_evaluator = medmnist.Evaluator(data_flag, 'test')\n",
        "\n",
        "    if task == \"multi-label, binary-class\":\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "    else:\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if model_path is not None:\n",
        "        model.load_state_dict(torch.load(model_path, map_location=device)['net'], strict=True)\n",
        "        train_metrics = test(model, train_evaluator, train_loader_at_eval, task, criterion, device, run, output_root)\n",
        "        val_metrics = test(model, val_evaluator, val_loader, task, criterion, device, run, output_root)\n",
        "        test_metrics = test(model, test_evaluator, test_loader, task, criterion, device, run, output_root)\n",
        "\n",
        "        print('train  auc: %.5f  acc: %.5f\\n' % (train_metrics[1], train_metrics[2]) + \\\n",
        "              'val  auc: %.5f  acc: %.5f\\n' % (val_metrics[1], val_metrics[2]) + \\\n",
        "              'test  auc: %.5f  acc: %.5f\\n' % (test_metrics[1], test_metrics[2]))\n",
        "\n",
        "    if num_epochs == 0:\n",
        "        return\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
        "\n",
        "    logs = ['loss', 'auc', 'acc']\n",
        "    train_logs = ['train_'+log for log in logs]\n",
        "    val_logs = ['val_'+log for log in logs]\n",
        "    test_logs = ['test_'+log for log in logs]\n",
        "    log_dict = OrderedDict.fromkeys(train_logs+val_logs+test_logs, 0)\n",
        "\n",
        "    writer = SummaryWriter(log_dir=os.path.join(output_root, 'Tensorboard_Results'))\n",
        "\n",
        "    best_auc = 0\n",
        "    best_epoch = 0\n",
        "    best_model = deepcopy(model)\n",
        "\n",
        "    global iteration\n",
        "    iteration = 0\n",
        "\n",
        "    for epoch in trange(num_epochs):\n",
        "        train_loss = train(model, train_loader, task, criterion, optimizer, device, writer)\n",
        "\n",
        "        train_metrics = test(model, train_evaluator, train_loader_at_eval, task, criterion, device, run)\n",
        "        val_metrics = test(model, val_evaluator, val_loader, task, criterion, device, run)\n",
        "        test_metrics = test(model, test_evaluator, test_loader, task, criterion, device, run)\n",
        "\n",
        "        # 收集每轮迭代数据\n",
        "        train_loss_list.append(train_loss)\n",
        "        train_auc_list.append(train_metrics[1])\n",
        "        train_acc_list.append(train_metrics[2])\n",
        "        test_loss_list.append(test_metrics[0])\n",
        "        test_auc_list.append(test_metrics[1])\n",
        "        test_acc_list.append(test_metrics[2])\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        for i, key in enumerate(train_logs):\n",
        "            log_dict[key] = train_metrics[i]\n",
        "        for i, key in enumerate(val_logs):\n",
        "            log_dict[key] = val_metrics[i]\n",
        "        for i, key in enumerate(test_logs):\n",
        "            log_dict[key] = test_metrics[i]\n",
        "\n",
        "        for key, value in log_dict.items():\n",
        "            writer.add_scalar(key, value, epoch)\n",
        "\n",
        "        cur_auc = val_metrics[1]\n",
        "        if cur_auc > best_auc:\n",
        "            best_epoch = epoch\n",
        "            best_auc = cur_auc\n",
        "            best_model = deepcopy(model)\n",
        "            print('cur_best_auc:', best_auc)\n",
        "            print('cur_best_epoch', best_epoch)\n",
        "\n",
        "    state = {\n",
        "        'net': best_model.state_dict(),\n",
        "    }\n",
        "\n",
        "    path = os.path.join(output_root, 'best_model.pth')\n",
        "    torch.save(state, path)\n",
        "\n",
        "    train_metrics = test(best_model, train_evaluator, train_loader_at_eval, task, criterion, device, run, output_root)\n",
        "    val_metrics = test(best_model, val_evaluator, val_loader, task, criterion, device, run, output_root)\n",
        "    test_metrics = test(best_model, test_evaluator, test_loader, task, criterion, device, run, output_root)\n",
        "\n",
        "    train_log = 'train  auc: %.5f  acc: %.5f\\n' % (train_metrics[1], train_metrics[2])\n",
        "    val_log = 'val  auc: %.5f  acc: %.5f\\n' % (val_metrics[1], val_metrics[2])\n",
        "    test_log = 'test  auc: %.5f  acc: %.5f\\n' % (test_metrics[1], test_metrics[2])\n",
        "\n",
        "    log = '%s\\n' % (data_flag) + train_log + val_log + test_log\n",
        "    print(log)\n",
        "\n",
        "    with open(os.path.join(output_root, '%s_log.txt' % (data_flag)), 'a') as f:\n",
        "        f.write(log)\n",
        "\n",
        "    writer.close()\n",
        "\n",
        "\n",
        "def train(model, train_loader, task, criterion, optimizer, device, writer):\n",
        "    total_loss = []\n",
        "    global iteration\n",
        "\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.to(device))\n",
        "\n",
        "        if task == 'multi-label, binary-class':\n",
        "            targets = targets.to(torch.float32).to(device)\n",
        "            loss = criterion(outputs, targets)\n",
        "        else:\n",
        "            targets = torch.squeeze(targets, 1).long().to(device)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        total_loss.append(loss.item())\n",
        "        writer.add_scalar('train_loss_logs', loss.item(), iteration)\n",
        "        iteration += 1\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = sum(total_loss)/len(total_loss)\n",
        "    return epoch_loss\n",
        "\n",
        "\n",
        "def test(model, evaluator, data_loader, task, criterion, device, run, save_folder=None):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = []\n",
        "    y_score = torch.tensor([]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "            outputs = model(inputs.to(device))\n",
        "\n",
        "            if task == 'multi-label, binary-class':\n",
        "                targets = targets.to(torch.float32).to(device)\n",
        "                loss = criterion(outputs, targets)\n",
        "                m = nn.Sigmoid()\n",
        "                outputs = m(outputs).to(device)\n",
        "            else:\n",
        "                targets = torch.squeeze(targets, 1).long().to(device)\n",
        "                loss = criterion(outputs, targets)\n",
        "                m = nn.Softmax(dim=1)\n",
        "                outputs = m(outputs).to(device)\n",
        "                targets = targets.float().resize_(len(targets), 1)\n",
        "\n",
        "            total_loss.append(loss.item())\n",
        "            y_score = torch.cat((y_score, outputs), 0)\n",
        "\n",
        "        y_score = y_score.detach().cpu().numpy()\n",
        "        auc, acc = evaluator.evaluate(y_score, save_folder, run)\n",
        "\n",
        "        test_loss = sum(total_loss) / len(total_loss)\n",
        "\n",
        "        return [test_loss, auc, acc]\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description='RUN Baseline model of MedMNIST2D')\n",
        "\n",
        "    parser.add_argument('--data_flag',\n",
        "                        default='pathmnist',\n",
        "                        type=str)\n",
        "    parser.add_argument('--output_root',\n",
        "                        default='./output',\n",
        "                        help='output root, where to save models and results',\n",
        "                        type=str)\n",
        "    parser.add_argument('--num_epochs',\n",
        "                        default=100,\n",
        "                        help='num of epochs of training, the script would only test model if set num_epochs to 0',\n",
        "                        type=int)\n",
        "    parser.add_argument('--gpu_ids',\n",
        "                        default='0',\n",
        "                        type=str)\n",
        "    parser.add_argument('--batch_size',\n",
        "                        default=128,\n",
        "                        type=int)\n",
        "    parser.add_argument('--download',\n",
        "                        action=\"store_true\")\n",
        "    parser.add_argument('--resize',\n",
        "                        help='resize images of size 28x28 to 224x224',\n",
        "                        action=\"store_true\")\n",
        "    parser.add_argument('--as_rgb',\n",
        "                        help='convert the grayscale image to RGB',\n",
        "                        action=\"store_true\")\n",
        "    parser.add_argument('--model_path',\n",
        "                        default=None,\n",
        "                        help='root of the pretrained model to test',\n",
        "                        type=str)\n",
        "    parser.add_argument('--model_flag',\n",
        "                        default='resnet18',\n",
        "                        help='choose backbone from resnet18, resnet50',\n",
        "                        type=str)\n",
        "    parser.add_argument('--run',\n",
        "                        default='model1',\n",
        "                        help='to name a standard evaluation csv file, named as {flag}_{split}_[AUC]{auc:.3f}_[ACC]{acc:.3f}@{run}.csv',\n",
        "                        type=str)\n",
        "\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    data_flag = args.data_flag\n",
        "    output_root = args.output_root\n",
        "    num_epochs = args.num_epochs\n",
        "    gpu_ids = args.gpu_ids\n",
        "    batch_size = args.batch_size\n",
        "    download = args.download\n",
        "    model_flag = args.model_flag\n",
        "    resize = args.resize\n",
        "    as_rgb = args.as_rgb\n",
        "    model_path = args.model_path\n",
        "    run = args.run\n",
        "\n",
        "    # data_flag = 'pathmnist'\n",
        "    # output_root = './output'\n",
        "    # num_epochs = 10\n",
        "    # gpu_ids = '0'\n",
        "    # batch_size = 128\n",
        "    # download = True\n",
        "    # model_flag = 'resnet18'\n",
        "    # resize = True\n",
        "    # as_rgb = True\n",
        "    # model_path = None\n",
        "    # run = 'model1'\n",
        "\n",
        "    main(data_flag, output_root, num_epochs, gpu_ids, batch_size, download, model_flag, resize, as_rgb, model_path, run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfW2Evt3Rhri"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# 绘制损失图\n",
        "plt.figure(figsize=(5, 4))\n",
        "plt.plot(train_loss_list, label='Training Loss')\n",
        "plt.plot(test_loss_list, label='Testing Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss vs Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 绘制 AUC 图\n",
        "plt.figure(figsize=(5, 4))\n",
        "plt.plot(train_auc_list, label='Training AUC')  # 直接使用 train_auc_list\n",
        "plt.plot(test_auc_list, label='Testing AUC')    # 直接使用 test_auc_list\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('AUC')\n",
        "plt.title('AUC vs Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 绘制准确率图\n",
        "plt.figure(figsize=(5, 4))\n",
        "plt.plot(train_acc_list, label='Training Accuracy')  # 使用 train_acc_list\n",
        "plt.plot(test_acc_list, label='Testing Accuracy')    # 使用 test_acc_list\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy vs Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Shap\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "batch = next(iter(train_loader))\n",
        "images, labels = batch\n",
        "\n",
        "background = images[:61].to(device)\n",
        "test_images = images[61:64].to(device)\n",
        "model.to(device)\n",
        "e = shap.DeepExplainer(model, background)\n",
        "shap_values = e.shap_values(test_images)\n",
        "\n",
        "shap_numpy = [np.swapaxes(np.swapaxes(s, 1, -1), 1, 2) for s in shap_values]\n",
        "test_numpy = np.swapaxes(np.swapaxes(test_images.detach().cpu().numpy(), 1, -1), 1, 2)\n",
        "\n",
        "# plot the feature attributions\n",
        "shap.image_plot(shap_numpy, test_numpy)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
